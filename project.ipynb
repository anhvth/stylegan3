{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d7234664-3253-474c-a96c-5743d29281a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "66572a61-a715-42de-a585-269fa6d96c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avcv.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ffbd384-d99d-4e00-a79e-f24dbc5484ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from time import perf_counter\n",
    "\n",
    "import click\n",
    "import imageio\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dnnlib\n",
    "import legacy\n",
    "# Load VGG16 feature detector.\n",
    "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "device = 'cuda:0'\n",
    "with dnnlib.util.open_url(url) as f:\n",
    "    vgg16 = torch.jit.load(f).eval().to(device)\n",
    "\n",
    "    \n",
    "def project(\n",
    "    G,\n",
    "    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
    "    *,\n",
    "    num_steps                  = 1000,\n",
    "    w_avg_samples              = 10000,\n",
    "    initial_learning_rate      = 0.1,\n",
    "    initial_noise_factor       = 0.05,\n",
    "    lr_rampdown_length         = 0.25,\n",
    "    lr_rampup_length           = 0.05,\n",
    "    noise_ramp_length          = 0.75,\n",
    "    regularize_noise_weight    = 1e5,\n",
    "    verbose                    = False,\n",
    "    extract_fn = None,\n",
    "    device: torch.device\n",
    "):\n",
    "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
    "\n",
    "    def logprint(*args):\n",
    "        if verbose:\n",
    "            print(*args)\n",
    "\n",
    "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n",
    "\n",
    "    # Compute w stats.\n",
    "    logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
    "    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
    "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
    "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n",
    "    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
    "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
    "\n",
    "    # Setup noise inputs.\n",
    "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
    "\n",
    "    # Load VGG16 feature detector.\n",
    "    # url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "\n",
    "    # Features for target image.\n",
    "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
    "    if target_images.shape[2] > 256:\n",
    "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
    "    # target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
    "    target_features = extract_fn(target_images)#, resize_images=False, return_lpips=True)\n",
    "\n",
    "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
    "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
    "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
    "\n",
    "    # Init noise.\n",
    "    for buf in noise_bufs.values():\n",
    "        buf[:] = torch.randn_like(buf)\n",
    "        buf.requires_grad = True\n",
    "        \n",
    "    pbar = tqdm(range(num_steps))\n",
    "    for step in pbar:\n",
    "        # Learning rate schedule.\n",
    "        t = step / num_steps\n",
    "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
    "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
    "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
    "        lr = initial_learning_rate * lr_ramp\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Synth images from opt_w.\n",
    "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
    "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
    "        synth_images = G.synthesis(ws, noise_mode='const')\n",
    "\n",
    "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "        synth_images = (synth_images + 1) * (255/2)\n",
    "        if synth_images.shape[2] > 256:\n",
    "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
    "\n",
    "        # Features for synth images.\n",
    "        # synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
    "        synth_features = extract_fn(synth_images)#, resize_images=False, return_lpips=True)\n",
    "        dist = (target_features - synth_features).square().sum()\n",
    "\n",
    "        # Noise regularization.\n",
    "        reg_loss = 0.0\n",
    "        for v in noise_bufs.values():\n",
    "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
    "            while True:\n",
    "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
    "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
    "                if noise.shape[2] <= 8:\n",
    "                    break\n",
    "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
    "        loss = dist + reg_loss * regularize_noise_weight\n",
    "\n",
    "        # Step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
    "\n",
    "        # Save projected W for each optimization step.\n",
    "        w_out[step] = w_opt.detach()[0]\n",
    "\n",
    "        # Normalize noise.\n",
    "        with torch.no_grad():\n",
    "            for buf in noise_bufs.values():\n",
    "                buf -= buf.mean()\n",
    "                buf *= buf.square().mean().rsqrt()\n",
    "\n",
    "    return w_out.repeat([1, G.mapping.num_ws, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8b3548d9-81f3-4d04-aa4f-73bc95c9c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_pkl = 'training-runs/00006-stylegan2-cropfaces-gpus6-batch24-gamma6.6/network-snapshot-004869.pkl'\n",
    "# target_fname = '/shared/RLDD/Fold1_part1_05_10/face_only/images/000052.jpg'\n",
    "target_fname = ''\n",
    "outdir='results/projected_517'\n",
    "save_video=True\n",
    "seed=0\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0472626-a281-4a65-a0ba-102c199431b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/anhvth8/YOLOX/lit_classifier/eyestate_prediction/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc24045b-f7ba-45a4-abba-c994c40b0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /home/anhvth8/YOLOX/lit_classifier/eyestate_prediction/\n",
    "from pretrained_networks import pretrained_factory\n",
    "# if not 'clip' in dir():\n",
    "# clip = pretrained_factory('clip').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fef62155-58b1-4cc7-a5f7-61758c4ac957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "model = clip.load(\"ViT-B/32\", device=device)[0]\n",
    "model = model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7998df2-49c8-4510-b7f4-788334a89b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "model = timm.create_model('resnet50', pretrained=True).to(device).eval().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "98222bcf-7c3e-4079-945a-fcb65b8ce7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc(torch)\n",
    "from torchvision import transforms\n",
    "normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "724713ab-afeb-458d-943c-c0a31bd514e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_feature_clip(inputs):\n",
    "    inputs = F.interpolate(inputs.to(device), size=(224,224))\n",
    "    inputs = inputs/255\n",
    "    inputs = normalize(inputs)\n",
    "    return model.encode_image(inputs)\n",
    "\n",
    "def extract_feature_vgg(inputs):\n",
    "    target_features = vgg16(inputs, resize_images=False, return_lpips=True)\n",
    "    return target_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cfe2ed91-3056-419a-b80c-be764b3ade13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2b54676e-85c7-4008-9001-fce95459ae0f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"training-runs/00006-stylegan2-cropfaces-gpus6-batch24-gamma6.6/network-snapshot-003346.pkl\"...\n",
      "Computing W midpoint and stddev using 10000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step  256/256: dist 0.16 loss 0.16 : 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:20<00:00, 12.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 21.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load networks.\n",
    "print('Loading networks from \"%s\"...' % network_pkl)\n",
    "if not 'G' in dir():\n",
    "    with dnnlib.util.open_url(network_pkl) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n",
    "\n",
    "# Load target image.\n",
    "# target_pil = PIL.Image.open(target_fname).convert('RGB')\n",
    "# target_pil = PIL.Image.open(target_fname).convert('GRAY')\n",
    "\n",
    "targets = []\n",
    "target_fnames = list(sorted(glob(os.path.join(input_dir, '*'))))[:1]\n",
    "target_uint8s = []\n",
    "# for target_fname in target_fnames:\n",
    "# img = cv2.imread(target_fname, 0)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "img = mmcv.imread(target_fname, channel_order='rgb')\n",
    "img = mmcv.imrescale(img, (256,256))\n",
    "img = mmcv.impad(img, shape=(256,256))\n",
    "\n",
    "target_pil = PIL.Image.fromarray(img)\n",
    "w, h = target_pil.size\n",
    "s = min(w, h)\n",
    "target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
    "target_pil = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)\n",
    "target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
    "targets.append(torch.tensor(target_uint8.transpose([2, 0, 1]), device=device))\n",
    "target_uint8s.append(target_uint8)\n",
    "    \n",
    "    \n",
    "# targets = torch.stack(targets)\n",
    "\n",
    "# Optimize projection.\n",
    "start_time = perf_counter()\n",
    "projected_w_steps = project(\n",
    "    G,\n",
    "    target=targets[0], # pylint: disable=not-callable\n",
    "    num_steps=num_steps,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    extract_fn=extract_feature_vgg,\n",
    ")\n",
    "\n",
    "print (f'Elapsed: {(perf_counter()-start_time):.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2ee19aa7-f010-4654-8440-e104796ced34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimization progress video /home/anhvth8/gitprojects/stylegan3/results/projected_517/proj.mp4\n"
     ]
    }
   ],
   "source": [
    "# Render debug output: optional video and projected image and W vector.\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "if save_video:\n",
    "    video = imageio.get_writer(f'{outdir}/proj.mp4', mode='I', fps=10, codec='libx264', bitrate='16M')\n",
    "    out_video_path = osp.abspath(f\"{outdir}/proj.mp4\")\n",
    "    print (f'Saving optimization progress video {out_video_path}')\n",
    "    for projected_w in projected_w_steps:\n",
    "        synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
    "        synth_image = (synth_image + 1) * (255/2)\n",
    "        synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "        video.append_data(np.concatenate([target_uint8s[0], synth_image], axis=1))\n",
    "    video.close()\n",
    "\n",
    "# Save final projected frame and W vector.\n",
    "target_pil.save(f'{outdir}/target.png')\n",
    "projected_w = projected_w_steps[-1]\n",
    "synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
    "synth_image = (synth_image + 1) * (255/2)\n",
    "synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/proj.png')\n",
    "np.savez(f'{outdir}/projected_w.npz', w=projected_w.unsqueeze(0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60834040-ec67-44f9-ae9a-1d51d71f5629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f7d2a-e4d1-42c1-9a67-586ed619d932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73cc697-e9fe-46dd-bbcd-8f2d989960a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
