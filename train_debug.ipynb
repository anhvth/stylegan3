{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe349e4-854b-4de3-bfa2-4e1700f0e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6bf147-ca91-4af3-9480-de04b5fcde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEBUG'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9867df-6338-466e-ae59-243085be1cff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "Training options:\n",
      "{\n",
      "  \"G_kwargs\": {\n",
      "    \"class_name\": \"training.networks_stylegan2.Generator\",\n",
      "    \"z_dim\": 512,\n",
      "    \"w_dim\": 512,\n",
      "    \"mapping_kwargs\": {\n",
      "      \"num_layers\": 8\n",
      "    },\n",
      "    \"channel_base\": 16384,\n",
      "    \"channel_max\": 512,\n",
      "    \"fused_modconv_default\": \"inference_only\"\n",
      "  },\n",
      "  \"D_kwargs\": {\n",
      "    \"class_name\": \"training.networks_stylegan2.Discriminator\",\n",
      "    \"block_kwargs\": {\n",
      "      \"freeze_layers\": 0\n",
      "    },\n",
      "    \"mapping_kwargs\": {},\n",
      "    \"epilogue_kwargs\": {\n",
      "      \"mbstd_group_size\": 4\n",
      "    },\n",
      "    \"channel_base\": 16384,\n",
      "    \"channel_max\": 512\n",
      "  },\n",
      "  \"G_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"betas\": [\n",
      "      0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"lr\": 0.002\n",
      "  },\n",
      "  \"D_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"betas\": [\n",
      "      0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"lr\": 0.002\n",
      "  },\n",
      "  \"loss_kwargs\": {\n",
      "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
      "    \"r1_gamma\": 6.6,\n",
      "    \"style_mixing_prob\": 0.9,\n",
      "    \"pl_weight\": 2,\n",
      "    \"pl_no_weight_grad\": true\n",
      "  },\n",
      "  \"data_loader_kwargs\": {\n",
      "    \"pin_memory\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"num_workers\": 3\n",
      "  },\n",
      "  \"training_set_kwargs\": {\n",
      "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
      "    \"path\": \"./data/images256x256/\",\n",
      "    \"use_labels\": false,\n",
      "    \"max_size\": 70000,\n",
      "    \"xflip\": true,\n",
      "    \"resolution\": 256,\n",
      "    \"random_seed\": 0\n",
      "  },\n",
      "  \"num_gpus\": 1,\n",
      "  \"batch_size\": 4,\n",
      "  \"batch_gpu\": 4,\n",
      "  \"metrics\": [],\n",
      "  \"total_kimg\": 5000,\n",
      "  \"kimg_per_tick\": 4,\n",
      "  \"image_snapshot_ticks\": 20,\n",
      "  \"network_snapshot_ticks\": 20,\n",
      "  \"random_seed\": 0,\n",
      "  \"ema_kimg\": 1.25,\n",
      "  \"G_reg_interval\": 4,\n",
      "  \"augment_kwargs\": {\n",
      "    \"class_name\": \"training.augment.AugmentPipe\",\n",
      "    \"xflip\": 1,\n",
      "    \"rotate90\": 1,\n",
      "    \"xint\": 1,\n",
      "    \"scale\": 1,\n",
      "    \"rotate\": 1,\n",
      "    \"aniso\": 1,\n",
      "    \"xfrac\": 1,\n",
      "    \"brightness\": 1,\n",
      "    \"contrast\": 1,\n",
      "    \"lumaflip\": 1,\n",
      "    \"hue\": 1,\n",
      "    \"saturation\": 1\n",
      "  },\n",
      "  \"ada_target\": 0.6,\n",
      "  \"run_dir\": \"./training-runs/debug/00015-stylegan2--gpus1-batch4-gamma6.6\"\n",
      "}\n",
      "\n",
      "Output directory:    ./training-runs/debug/00015-stylegan2--gpus1-batch4-gamma6.6\n",
      "Number of GPUs:      1\n",
      "Batch size:          4 images\n",
      "Training duration:   5000 kimg\n",
      "Dataset path:        ./data/images256x256/\n",
      "Dataset size:        70000 images\n",
      "Dataset resolution:  256\n",
      "Dataset labels:      False\n",
      "Dataset x-flips:     True\n",
      "\n",
      "Creating output directory...\n",
      "Launching processes...\n",
      "Loading training set...\n",
      "\n",
      "Num images:  140000\n",
      "Image shape: [3, 256, 256]\n",
      "Label shape: [0]\n",
      "\n",
      "Constructing networks...\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "\n",
      "Generator             Parameters  Buffers  Output shape        Datatype\n",
      "---                   ---         ---      ---                 ---     \n",
      "mapping.fc0           262656      -        [4, 512]            float32 \n",
      "mapping.fc1           262656      -        [4, 512]            float32 \n",
      "mapping.fc2           262656      -        [4, 512]            float32 \n",
      "mapping.fc3           262656      -        [4, 512]            float32 \n",
      "mapping.fc4           262656      -        [4, 512]            float32 \n",
      "mapping.fc5           262656      -        [4, 512]            float32 \n",
      "mapping.fc6           262656      -        [4, 512]            float32 \n",
      "mapping.fc7           262656      -        [4, 512]            float32 \n",
      "mapping               -           512      [4, 14, 512]        float32 \n",
      "synthesis.b4.conv1    2622465     32       [4, 512, 4, 4]      float32 \n",
      "synthesis.b4.torgb    264195      -        [4, 3, 4, 4]        float32 \n",
      "synthesis.b4:0        8192        16       [4, 512, 4, 4]      float32 \n",
      "synthesis.b4:1        -           -        [4, 3, 4, 4]        float32 \n",
      "synthesis.b8.conv0    2622465     80       [4, 512, 8, 8]      float32 \n",
      "synthesis.b8.conv1    2622465     80       [4, 512, 8, 8]      float32 \n",
      "synthesis.b8.torgb    264195      -        [4, 3, 8, 8]        float32 \n",
      "synthesis.b8:0        -           16       [4, 512, 8, 8]      float32 \n",
      "synthesis.b8:1        -           -        [4, 3, 8, 8]        float32 \n",
      "synthesis.b16.conv0   2622465     272      [4, 512, 16, 16]    float32 \n",
      "synthesis.b16.conv1   2622465     272      [4, 512, 16, 16]    float32 \n",
      "synthesis.b16.torgb   264195      -        [4, 3, 16, 16]      float32 \n",
      "synthesis.b16:0       -           16       [4, 512, 16, 16]    float32 \n",
      "synthesis.b16:1       -           -        [4, 3, 16, 16]      float32 \n",
      "synthesis.b32.conv0   2622465     1040     [4, 512, 32, 32]    float16 \n",
      "synthesis.b32.conv1   2622465     1040     [4, 512, 32, 32]    float16 \n",
      "synthesis.b32.torgb   264195      -        [4, 3, 32, 32]      float16 \n",
      "synthesis.b32:0       -           16       [4, 512, 32, 32]    float16 \n",
      "synthesis.b32:1       -           -        [4, 3, 32, 32]      float32 \n",
      "synthesis.b64.conv0   1442561     4112     [4, 256, 64, 64]    float16 \n",
      "synthesis.b64.conv1   721409      4112     [4, 256, 64, 64]    float16 \n",
      "synthesis.b64.torgb   132099      -        [4, 3, 64, 64]      float16 \n",
      "synthesis.b64:0       -           16       [4, 256, 64, 64]    float16 \n",
      "synthesis.b64:1       -           -        [4, 3, 64, 64]      float32 \n",
      "synthesis.b128.conv0  426369      16400    [4, 128, 128, 128]  float16 \n",
      "synthesis.b128.conv1  213249      16400    [4, 128, 128, 128]  float16 \n",
      "synthesis.b128.torgb  66051       -        [4, 3, 128, 128]    float16 \n",
      "synthesis.b128:0      -           16       [4, 128, 128, 128]  float16 \n",
      "synthesis.b128:1      -           -        [4, 3, 128, 128]    float32 \n",
      "synthesis.b256.conv0  139457      65552    [4, 64, 256, 256]   float16 \n",
      "synthesis.b256.conv1  69761       65552    [4, 64, 256, 256]   float16 \n",
      "synthesis.b256.torgb  33027       -        [4, 3, 256, 256]    float16 \n",
      "synthesis.b256:0      -           16       [4, 64, 256, 256]   float16 \n",
      "synthesis.b256:1      -           -        [4, 3, 256, 256]    float32 \n",
      "---                   ---         ---      ---                 ---     \n",
      "Total                 24767458    175568   -                   -       \n",
      "\n",
      "\n",
      "Discriminator  Parameters  Buffers  Output shape        Datatype\n",
      "---            ---         ---      ---                 ---     \n",
      "b256.fromrgb   256         16       [4, 64, 256, 256]   float16 \n",
      "b256.skip      8192        16       [4, 128, 128, 128]  float16 \n",
      "b256.conv0     36928       16       [4, 64, 256, 256]   float16 \n",
      "b256.conv1     73856       16       [4, 128, 128, 128]  float16 \n",
      "b256           -           16       [4, 128, 128, 128]  float16 \n",
      "b128.skip      32768       16       [4, 256, 64, 64]    float16 \n",
      "b128.conv0     147584      16       [4, 128, 128, 128]  float16 \n",
      "b128.conv1     295168      16       [4, 256, 64, 64]    float16 \n",
      "b128           -           16       [4, 256, 64, 64]    float16 \n",
      "b64.skip       131072      16       [4, 512, 32, 32]    float16 \n",
      "b64.conv0      590080      16       [4, 256, 64, 64]    float16 \n",
      "b64.conv1      1180160     16       [4, 512, 32, 32]    float16 \n",
      "b64            -           16       [4, 512, 32, 32]    float16 \n",
      "b32.skip       262144      16       [4, 512, 16, 16]    float16 \n",
      "b32.conv0      2359808     16       [4, 512, 32, 32]    float16 \n",
      "b32.conv1      2359808     16       [4, 512, 16, 16]    float16 \n",
      "b32            -           16       [4, 512, 16, 16]    float16 \n",
      "b16.skip       262144      16       [4, 512, 8, 8]      float32 \n",
      "b16.conv0      2359808     16       [4, 512, 16, 16]    float32 \n",
      "b16.conv1      2359808     16       [4, 512, 8, 8]      float32 \n",
      "b16            -           16       [4, 512, 8, 8]      float32 \n",
      "b8.skip        262144      16       [4, 512, 4, 4]      float32 \n",
      "b8.conv0       2359808     16       [4, 512, 8, 8]      float32 \n",
      "b8.conv1       2359808     16       [4, 512, 4, 4]      float32 \n",
      "b8             -           16       [4, 512, 4, 4]      float32 \n",
      "b4.mbstd       -           -        [4, 513, 4, 4]      float32 \n",
      "b4.conv        2364416     16       [4, 512, 4, 4]      float32 \n",
      "b4.fc          4194816     -        [4, 512]            float32 \n",
      "b4.out         513         -        [4, 1]              float32 \n",
      "---            ---         ---      ---                 ---     \n",
      "Total          24001089    416      -                   -       \n",
      "\n",
      "Setting up augmentation...\n",
      "Distributing across 1 GPUs...\n",
      "Setting up training phases...\n",
      "Initializing logs...\n",
      "Training for 5000 kimg...\n",
      "\n",
      "tick 0     kimg 0.0      time 33s          sec/tick 13.2    sec/kimg 3305.91 maintenance 20.0   cpumem 3.79   gpumem 9.71   reserved 16.96  augment 0.000\n",
      "tick 1     kimg 4.0      time 5m 40s       sec/tick 305.9   sec/kimg 76.48   maintenance 0.8    cpumem 4.34   gpumem 2.42   reserved 8.70   augment 0.007\n",
      "tick 2     kimg 8.0      time 10m 32s      sec/tick 292.4   sec/kimg 73.11   maintenance 0.0    cpumem 4.34   gpumem 2.43   reserved 8.70   augment 0.014\n",
      "tick 3     kimg 12.0     time 15m 20s      sec/tick 287.2   sec/kimg 71.79   maintenance 0.1    cpumem 4.34   gpumem 2.43   reserved 8.70   augment 0.021\n"
     ]
    }
   ],
   "source": [
    "%run train.py --metrics none  --cbase 16384 --data ./data/images256x256/ --gpus=1 --outdir=./training-runs/debug --cfg=stylegan2 --batch=4 --gamma=6.6 --mirror=1 --kimg=5000 --snap=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0378fc-8cd4-4581-aeaa-30fa6db8f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "# text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50bede17-53aa-4e27-8a01-db86186d39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(1,3,224,224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb071bb5-460d-43ff-912c-33e7ba0df975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_image(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d49346-d494-4d67-a9b9-c0c61bf04cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
